# -*- coding: utf-8 -*-

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn import preprocessing
import tensorflow as tf
from tensorflow import keras
from sklearn.neighbors import KernelDensity
from scipy.spatial import distance
# from scipy.stats import zscore
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.metrics import precision_recall_fscore_support

from LAKE import LVAE
from feature_selection import PFA

delta = 15
iteDay = 1440/delta
h = 5

def featureSelecting(f_df_raw):
    X = f_df_raw
    pfa = PFA(n_features=10)
    pfa.fit(X)
    
    # To get the transformed matrix
    X = pfa.features_
    
    # To get the column indices of the kept features
    column_indices = pfa.indices_
    
    column_names = f_df_raw.columns[column_indices]
    if '冰機Q(sum)' not in column_names:
        column_names = column_names.insert(0, '冰機Q(sum)')
    return column_names

def convertTime(timestamp):
    date = timestamp.apply(lambda _ : _.date())
    hour = timestamp.apply(lambda _ : _.hour)
    minute = timestamp.apply(lambda _ : _.minute)
    time = pd.DataFrame(data={'d':date.values, 'h':hour.values, 'm':minute.values}, index=timestamp)
    return time
    
def dataPreProcessing(f_df, time):
    if time is None:
        time = convertTime(f_df_raw.index)
    
    date = time['d'].loc[f_df.index]
    hour = time['h'].loc[f_df.index]
    minute = time['m'].loc[f_df.index]
    
    avg_FDate = f_df.groupby(
        [
          lambda i : date[i], 
          lambda i : ( hour[i] * 60 + minute[i] // delta )
        ]).mean()
    
    countDay1 = avg_FDate.groupby(level=0).apply(lambda _ : len(_))
    avg_FDate_drop = avg_FDate.drop((
        countDay1.loc[countDay1 != iteDay]
        ).index)
    countDay2 = avg_FDate_drop.groupby(level=0).apply(lambda _ : len(_))
    assert all(countDay2 == iteDay) # sum(countDay2 != iteDay) == 0
    unitFrames = avg_FDate_drop.unstack(level=1).dropna() # unitFrames.stack(level=1) # [Redo -> avg_FDate_drop]
    
    return unitFrames

def transferESpace(model, data):
    _, _, _, _, _, z = model.encoder.predict(data)
    _, _, _, reconstruction = model.decoder(z)
    r = np.array(
        [
          [distance.euclidean(i, j), distance.cosine(i, j)]
          for i, j in zip(tf.convert_to_tensor(train), reconstruction)])
    
    c = np.concatenate([z, r], axis=1)
    return c

def trainModel(unitFrames, file_name_save):
    model = LVAE(latent_dim = 6, fDim = unitFrames.shape[1])
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=3e-5))
    history = model.fit(unitFrames, epochs=324, batch_size=8)
    
    model.save_weights(file_name_save)
    model = LVAE(latent_dim = 6, fDim = unitFrames.shape[1])
    model.load_weights(file_name_save)
    return model, history

if __name__ == '__main__':
    
    tf.debugging.enable_check_numerics()
    tf.keras.backend.set_floatx('float64')
    tf.keras.backend.floatx()
    
    df_raw = pd.read_csv('./dataset/Merge.csv')
    non_dup_df_raw = df_raw[~df_raw['時間'].duplicated()].reset_index(drop=True)
    non_dup_f_df_raw = non_dup_df_raw.dropna()
    
    f_df_raw = non_dup_f_df_raw.copy()
    
    timestamp = f_df_raw.pop('時間').apply(lambda _ : pd.to_datetime(_))
    time = convertTime(timestamp)
    # print(time)
    f_df_raw.index = timestamp
    # print(f_df_raw)
    
    column_names = featureSelecting(f_df_raw)
    f_df = pd.DataFrame(f_df_raw[column_names], index=f_df_raw.index)
    
    transformer1 = preprocessing.RobustScaler().fit(f_df)
    f_df[:] = transformer1.transform(f_df)
    
    train = dataPreProcessing(f_df, time)
    
    f_df[:] = transformer1.inverse_transform(f_df)
    print(f_df)
    
    model, history = trainModel(train, './weightsModel/1209-1')
    
    c = transferESpace(model, train)
    kde = KernelDensity(kernel='gaussian', bandwidth=h).fit(c)
    k = pd.DataFrame(kde.score_samples(c), index=train.index, columns=['densityln'])
    
    # ---------------------------------------------
    
    err_raw = pd.read_csv('./dataset/err-0001.csv')
    
    timeErr_raw = err_raw['time'].apply(lambda _ : pd.to_datetime(_))
    
    non_duplicated = ~timeErr_raw.duplicated()
    err = err_raw[non_duplicated].dropna().reset_index(drop=True)
    timeErr = timeErr_raw[non_duplicated].dropna().reset_index(drop=True)
    
    time_err = convertTime(timeErr)
    # print(time_err)
    
    tlErr = pd.DataFrame({'time' : timeErr.values, 'label' : err['label'].values})
    
    f_err = err[column_names]
    
    groupedErr = f_err.groupby(
        [
          lambda i: time_err['d'][i], 
          lambda i : (60 * time_err['h'][i] + time_err['m'][i]) // 15
          ]
        )
    meanErr = groupedErr.mean()
    
    countDay1 = meanErr.groupby(level=0).apply(lambda _ : len(_))
    meanErr_drop = meanErr.drop((
        countDay1.loc[countDay1 != iteDay]
        ).index)
    
    countDay2 = meanErr_drop.groupby(level=0).apply(lambda _ : len(_))
    assert all(countDay2 == iteDay) # sum(countDay2 != iteDay) == 0
    unstackMErr = meanErr_drop.unstack(level=1).dropna() # unstackMErr.stack(level=1) # [Redo -> avg_FDate_drop]
    print(unstackMErr)
    
    unstackLabel = tlErr.groupby(lambda i: time_err['d'].iloc[i]).apply(lambda i : i['label'].any())
    
    test = unstackMErr
    
    c_test = transferESpace(model, test)
    k_test = pd.DataFrame(kde.score_samples(c_test), index=test.index, columns=['densityln'])
    
    # k = pd.DataFrame(kde.score_samples(c), index=train.index, columns=['densityln'])
    k_test['z'] = (pd.DataFrame(k_test.densityln) - k.densityln.mean()) / k_test.densityln.std()
    
    metric_Day = pd.DataFrame(data={
        'ground_true' : ~unstackLabel.loc[test.index], 
        'predict' : k_test.z
        })
    
    fig, axs = plt.subplots(2, figsize=(100, 15))
    axs[0].plot(timestamp, f_df_raw['冰機Q(sum)'], 'b.')
    axs[1].plot(timeErr[~err.label], f_err[~err.label]['冰機Q(sum)'], 'g.')
    axs[1].plot(timeErr[err.label], f_err[err.label]['冰機Q(sum)'], 'r.')
    plt.show()
    
    precision_recall_fscore_support(metric_Day.ground_true, metric_Day.predict > -3)
    
    fpr, tpr, thresholds = roc_curve(metric_Day.ground_true, metric_Day.predict, pos_label=1)
    print( roc_auc_score(metric_Day.ground_true, metric_Day.predict) )
    
    plt.plot(fpr, tpr)
    plt.show()
